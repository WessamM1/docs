---
title: Workloads
---

## [Overview](#overview)

A workload represents a backend application such as a microservice. It is comprised of one
or multiple containers. Containers making up the workload communicate freely on localhost.

Workloads run in Control Plane’s AWS, Azure, and GCP accounts, where clouds/regions are determined by
the GVC definition. Your workload may run only in a single region of one cloud, or across many regions of all the
three clouds – completely up to the GVC definition. Requests are routed to the nearest healthy location.

Workloads are managed using a common interface, regardless of cloud providers. Workload log data is
consolidated for easy retrieval and analysis. It means that a particular workload can be operating on
AWS, Azure and GCP, yet its log – across instances/providers is accessed using a single API/CLI/UI/Grafana operation.

## [Features](#features)

- [Auto Scaling](#auto-scaling)
- DNS geo-routing
- Intelligent resource management called [Capacity AI](#capacity-ai)
- Load balancing
- [Location specific override](#location-override) of scaling and resource management
- Logging
- [Probes](#probes)
- [Alerts](#alerts)

## [Auto Scaling](#auto-scaling)

The number of workload replicas is automatically scaled up and down based on the workload's scaling strategy.

Selectable Scaling Strategies:

- Disabled
- Concurrent Requests Quantity
- Requests Per Second
- Percentage of CPU Utilization
- Request Latency

The minimum and maximum number of replicas that can be deployed are configurable. Workloads can be scaled down to 0 when there is no
traffic and can scale up immediately to fulfil new request.

<Info>
[Capacity AI](#capacity-ai) is not available if CPU Utilization is selected because dynamic allocation of CPU resources cannot be
accomplished while scaling replicas based on the usage of its CPU.
</Info>

## [Capacity AI](#capacity-ai)

A workload can leverage intelligent allocation of its container's resources (CPU and Memory) by using Capacity AI.

Capacity AI uses an analysis of historical usage to adjust these resources up to a configured maximum.

This can significantly reduce cost but may cause temporary performance issues with sudden spikes in usage.

If capacity AI is disabled, the amount of resources configured will be fully allocated.

## [Location Override](#location-override)

By default, both [Capacity AI](#capacity-ai) and [Auto Scaling](#auto-scaling) settings are applied to all deployments at each locations
enabled in the GVC. Each location can have these settings overridden to increase performance for a particular audience.

This allows for granular control over how your workload scales for a particular location. If a majority of your users are in
Europe, you can set the European locations at a higher level than the rest of the world.

Setting local options will ensure that your target users will be served quickly and results in lower costs for resources that
won't be used.

## [Probes](#probes)

Probes are a feature of Kubernetes that are used to control the health of an application running inside a container.

Each container can have a:

- Readiness Probe

  - An endpoint configured which can be queried to see if the workload is available and ready to receive requests

- Liveness Probe
  - An endpoint configured which can be queried to see if the workload is healthy or if it needs to be restarted

## [Alerts](#alerts)

Using Grafana, you can create alerts on any of the standard metrics exposed by Control Plane, or on your [custom metrics](/api-reference/workload#metrics).
To access Grafana, navigate to one of your orgs in the Control Plane console and click the "Metrics" link.

You have the full capability of Grafana alerting at your disposal. For more information, please consult [the Grafana documentation](https://grafana.com/docs/grafana/latest/alerting/)

## [Reference](#reference)

Visit the [workload reference](/api-reference/workload) page for additional information.

## [Types](#types)

- **Serverless**:
  - Workloads that scale to zero when they aren't receiving requests.
- **Standard**:
  - Workloads that serve network traffic on multiple ports, but do not scale to zero.
- **Cron**:
  - Workloads that run on a schedule, and do not serve network traffic.
- **Stateful**:
  - Simlilar to a `standard` workload, `stateful` workloads have stable replica identities and hostnames, and can mount a [volume set](/api-reference/volumeset) for persistent storage.
